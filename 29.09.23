```
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2$        python -m nanoT5.main \
>            optim.name={adafactor,adamwscale} \
>            optim.lr_scheduler={legacy,cosine}
/home/berkin/anaconda3/envs/nanoT5_v2/bin/python: No module named nanoT5.main
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2$ cd /home/berkin/Desktop/nanoT5_v2/nanoT5/nanoT5
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5/nanoT5$        python -m nanoT5.main \
>            optim.name={adafactor,adamwscale} \
>            optim.lr_scheduler={legacy,cosine}
/home/berkin/anaconda3/envs/nanoT5_v2/bin/python: Error while finding module specification for 'nanoT5.main' (ModuleNotFoundError: No module named 'nanoT5')
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5/nanoT5$ cd ..
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5$        python -m nanoT5.main \
>            optim.name={adafactor,adamwscale} \
>            optim.lr_scheduler={legacy,cosine}
Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/berkin/.cache/huggingface/token
Login successful
[2023-09-29 16:01:11,766][Main][INFO] - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

[2023-09-29 16:01:11,766][Main][INFO] - Working directory is /home/berkin/Desktop/nanoT5_v2/nanoT5/logs/2023-09-29/16-01-11-
loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

loading file spiece.model from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/spiece.model
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/tokenizer_config.json
loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2023-09-29 16:01:29,126][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00890-of-01024.json.gz
[2023-09-29 16:01:29,126][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00720-of-01024.json.gz
[2023-09-29 16:01:29,126][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00174-of-01024.json.gz
[2023-09-29 16:01:29,126][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00654-of-01024.json.gz
[2023-09-29 16:01:29,126][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00011-of-01024.json.gz
[2023-09-29 16:01:29,127][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.01015-of-01024.json.gz
[2023-09-29 16:01:29,127][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00821-of-01024.json.gz
[2023-09-29 16:01:29,129][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00427-of-01024.json.gz
Error executing job with overrides: ['optim.name=adafactor', 'optim.name=adamwscale', 'optim.lr_scheduler=legacy', 'optim.lr_scheduler=cosine']
Traceback (most recent call last):
  File "/home/berkin/Desktop/nanoT5_v2/nanoT5/nanoT5/main.py", line 78, in main
    train(model, train_dataloader, test_dataloader, accelerator,
  File "/home/berkin/Desktop/nanoT5_v2/nanoT5/nanoT5/utils/train_utils.py", line 186, in train
    for batch_id, batch in enumerate(train_dataloader, start=1):
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/accelerate/data_loader.py", line 560, in __iter__
    next_batch, next_batch_info = self._fetch_batches(main_iterator)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/accelerate/data_loader.py", line 523, in _fetch_batches
    batches.append(next(iterator))
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 633, in __next__
    data = self._next_data()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/_utils.py", line 644, in reraise
    raise exception
FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 32, in fetch
    data.append(next(self.dataset_iter))
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 1358, in __iter__
    yield from self._iter_pytorch()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 1293, in _iter_pytorch
    for key, example in ex_iterable:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 982, in __iter__
    for x in self.ex_iterable:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 678, in __iter__
    yield from self._iter()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 693, in _iter
    for key, example in iterator:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 1114, in __iter__
    for key, example in self.ex_iterable:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 678, in __iter__
    yield from self._iter()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 740, in _iter
    for key, example in iterator:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 1114, in __iter__
    for key, example in self.ex_iterable:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 233, in __iter__
    yield from self.generate_examples_fn(**self.kwargs)
  File "/home/berkin/.cache/huggingface/modules/datasets_modules/datasets/c4/df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01/c4.py", line 88, in _generate_examples
    with gzip.open(open(filepath, "rb"), "rt", encoding="utf-8") as f:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/streaming.py", line 74, in wrapper
    return function(*args, download_config=download_config, **kwargs)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py", line 507, in xopen
    raise FileNotFoundError(
FileNotFoundError: https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00821-of-01024.json.gz
If the repo is private or gated, make sure to log in with `huggingface-cli login`.


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5$        python -m nanoT5.main \
>            optim.name={adafactor,adamwscale} \
>            optim.lr_scheduler={legacy,cosine}
Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/berkin/.cache/huggingface/token
Login successful
[2023-09-29 16:02:33,973][Main][INFO] - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

[2023-09-29 16:02:33,974][Main][INFO] - Working directory is /home/berkin/Desktop/nanoT5_v2/nanoT5/logs/2023-09-29/16-02-33-
loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

loading file spiece.model from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/spiece.model
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/tokenizer_config.json
loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

Downloading data files:   0%|                                                                                                             | 0/1024 [00:00<?, ?it/s]
Downloading data:   1%|â–Š                                                                                                       | 2.63M/319M [00:00<01:39, 3.18MB/s]
Traceback (most recent call last):
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/berkin/Desktop/nanoT5_v2/nanoT5/nanoT5/main.py", line 86, in <module>
    main()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/berkin/Desktop/nanoT5_v2/nanoT5/nanoT5/main.py", line 46, in main
    train_dataloader, test_dataloader = get_dataloaders(tokenizer, config, args)
  File "/home/berkin/Desktop/nanoT5_v2/nanoT5/nanoT5/utils/model_utils.py", line 245, in get_dataloaders
    dataset_splits = load_dataset_splits(args)
  File "/home/berkin/Desktop/nanoT5_v2/nanoT5/nanoT5/utils/model_utils.py", line 141, in load_dataset_splits
    dataset = datasets.load_dataset(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/load.py", line 2153, in load_dataset
    builder_instance.download_and_prepare(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/builder.py", line 954, in download_and_prepare
    self._download_and_prepare(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/builder.py", line 1717, in _download_and_prepare
    super()._download_and_prepare(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/builder.py", line 1027, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File "/home/berkin/.cache/huggingface/modules/datasets_modules/datasets/c4/df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01/c4.py", line 74, in _split_generators
    train_downloaded_files = dl_manager.download(data_urls["train"])
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/download/download_manager.py", line 428, in download
    downloaded_path_or_paths = map_nested(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/utils/py_utils.py", line 464, in map_nested
    mapped = [
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/utils/py_utils.py", line 465, in <listcomp>
    _single_map_nested((function, obj, types, None, True, None))
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/utils/py_utils.py", line 367, in _single_map_nested
    return function(data_struct)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/download/download_manager.py", line 454, in _download
    return cached_path(url_or_filename, download_config=download_config)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/utils/file_utils.py", line 182, in cached_path
    output_path = get_from_cache(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/utils/file_utils.py", line 644, in get_from_cache
    http_get(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/utils/file_utils.py", line 419, in http_get
    for chunk in response.iter_content(chunk_size=1024):
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/requests/models.py", line 816, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/response.py", line 628, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/response.py", line 567, in read
    data = self._fp_read(amt) if not fp_closed else b""
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/response.py", line 533, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/http/client.py", line 459, in read
    n = self.readinto(b)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/http/client.py", line 503, in readinto
    n = self.fp.readinto(b)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/socket.py", line 669, in readinto
    return self._sock.recv_into(b)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/ssl.py", line 1274, in recv_into
^C  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/ssl.py", line 1132, in read
    return self._sslobj.read(len, buffer)
KeyboardInterrupt
Downloading data files:   0%|                                                                                                             | 0/1024 [00:03<?, ?it/s]
^C
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5$ ^C
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5$ ^C
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5$ 


```
