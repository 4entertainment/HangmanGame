```

(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5$        python -m nanoT5.main \
>            optim.name={adafactor,adamwscale} \
>            optim.lr_scheduler={legacy,cosine}
Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /home/berkin/.cache/huggingface/token
Login successful
[2023-09-29 16:14:07,780][Main][INFO] - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

[2023-09-29 16:14:07,781][Main][INFO] - Working directory is /home/berkin/Desktop/nanoT5_v2/nanoT5/logs/2023-09-29/16-14-07-
loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

loading file spiece.model from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/spiece.model
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/tokenizer_config.json
loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
loading configuration file config.json from cache at /home/berkin/.cache/huggingface/hub/models--google--t5-v1_1-base/snapshots/b5fc947a416ea3cb079532cb3c2bbadeb7f800fc/config.json
Model config T5Config {
  "_name_or_path": "google/t5-v1_1-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.33.3",
  "use_cache": true,
  "vocab_size": 32128
}

[2023-09-29 16:14:16,368][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00654-of-01024.json.gz
[2023-09-29 16:14:16,368][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00821-of-01024.json.gz
[2023-09-29 16:14:16,369][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.01015-of-01024.json.gz
[2023-09-29 16:14:16,370][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00427-of-01024.json.gz
[2023-09-29 16:14:16,370][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00720-of-01024.json.gz
[2023-09-29 16:14:16,370][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00011-of-01024.json.gz
[2023-09-29 16:14:16,370][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00890-of-01024.json.gz
[2023-09-29 16:14:16,370][datasets_modules.datasets.c4.df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01.c4][INFO] - generating examples from = https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00174-of-01024.json.gz
Error executing job with overrides: ['optim.name=adafactor', 'optim.name=adamwscale', 'optim.lr_scheduler=legacy', 'optim.lr_scheduler=cosine']
Traceback (most recent call last):
  File "/home/berkin/Desktop/nanoT5_v2/nanoT5/nanoT5/main.py", line 78, in main
    train(model, train_dataloader, test_dataloader, accelerator,
  File "/home/berkin/Desktop/nanoT5_v2/nanoT5/nanoT5/utils/train_utils.py", line 186, in train
    for batch_id, batch in enumerate(train_dataloader, start=1):
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/accelerate/data_loader.py", line 560, in __iter__
    next_batch, next_batch_info = self._fetch_batches(main_iterator)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/accelerate/data_loader.py", line 523, in _fetch_batches
    batches.append(next(iterator))
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 633, in __next__
    data = self._next_data()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/_utils.py", line 644, in reraise
    raise exception
FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 32, in fetch
    data.append(next(self.dataset_iter))
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 1358, in __iter__
    yield from self._iter_pytorch()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 1293, in _iter_pytorch
    for key, example in ex_iterable:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 982, in __iter__
    for x in self.ex_iterable:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 678, in __iter__
    yield from self._iter()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 693, in _iter
    for key, example in iterator:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 1114, in __iter__
    for key, example in self.ex_iterable:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 678, in __iter__
    yield from self._iter()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 740, in _iter
    for key, example in iterator:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 1114, in __iter__
    for key, example in self.ex_iterable:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/iterable_dataset.py", line 233, in __iter__
    yield from self.generate_examples_fn(**self.kwargs)
  File "/home/berkin/.cache/huggingface/modules/datasets_modules/datasets/c4/df532b158939272d032cc63ef19cd5b83e9b4d00c922b833e4cb18b2e9869b01/c4.py", line 88, in _generate_examples
    with gzip.open(open(filepath, "rb"), "rt", encoding="utf-8") as f:
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/streaming.py", line 74, in wrapper
    return function(*args, download_config=download_config, **kwargs)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py", line 507, in xopen
    raise FileNotFoundError(
FileNotFoundError: https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00821-of-01024.json.gz
If the repo is private or gated, make sure to log in with `huggingface-cli login`.


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5$ huggingface-cli
usage: huggingface-cli <command> [<args>]

positional arguments:
  {env,login,whoami,logout,repo,upload,download,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache}
                        huggingface-cli command helpers
    env                 Print information about the environment.
    login               Log in using a token from huggingface.co/settings/tokens
    whoami              Find out which huggingface.co account you are logged in as.
    logout              Log out
    repo                {create, ls-files} Commands to interact with your huggingface.co repos.
    upload              Upload a file or a folder to a repo on the Hub
    download            Download files from the Hub
    lfs-enable-largefiles
                        Configure your repository to enable upload of files > 5GB.
    lfs-multipart-upload
                        Command will get called by git-lfs, do not call it directly.
    scan-cache          Scan cache directory.
    delete-cache        Delete revisions from the cache directory.

optional arguments:
  -h, --help            show this help message and exit
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5$ huggingface-cli login

    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|
    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|
    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|
    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|
    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|
    
    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.
    Setting a new token will erase the existing one.
    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .
Token: 
Add token as git credential? (Y/n) y
Traceback (most recent call last):
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/connectionpool.py", line 714, in urlopen
    httplib_response = self._make_request(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/connectionpool.py", line 403, in _make_request
    self._validate_conn(conn)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/connectionpool.py", line 1053, in _validate_conn
    conn.connect()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/connection.py", line 419, in connect
    self.sock = ssl_wrap_socket(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/ssl.py", line 1073, in _create
    self.do_handshake()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/ssl.py", line 1342, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1131)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/connectionpool.py", line 798, in urlopen
    retries = retries.increment(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/whoami-v2 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1131)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/berkin/anaconda3/envs/nanoT5_v2/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/huggingface_hub/commands/huggingface_cli.py", line 49, in main
    service.run()
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/huggingface_hub/commands/user.py", line 101, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/huggingface_hub/_login.py", line 100, in login
    interpreter_login(new_session=new_session, write_permission=write_permission)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/huggingface_hub/_login.py", line 164, in interpreter_login
    _login(token=token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/huggingface_hub/_login.py", line 273, in _login
    permission = get_token_permission(token)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/huggingface_hub/hf_api.py", line 978, in get_token_permission
    return self.whoami(token=token)["auth"]["accessToken"]["role"]
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/huggingface_hub/hf_api.py", line 945, in whoami
    r = get_session().get(
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/huggingface_hub/utils/_http.py", line 63, in send
    return super().send(request, *args, **kwargs)
  File "/home/berkin/anaconda3/envs/nanoT5_v2/lib/python3.8/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/whoami-v2 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1131)')))"), '(Request ID: 22668491-844f-4ff6-adc3-0c0148fea3b6)')
(nanoT5_v2) berkin@berkin:~/Desktop/nanoT5_v2/nanoT5$ 


```
